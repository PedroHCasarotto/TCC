# TransGEM

Transformer-based model from gene expression to molecules.

TransGEM is a phenotype-based de novo drug design model, which can generate new bioactive molecules, independent from disease target information.

![Graphical abstract](https://github.com/hzauzqy/TransGEM/blob/main/assets/Figure1.png "Graphical abstract")



# Setup
### Install the environment

- Create a conda environment:
```sh
conda env create -f environment.yaml
```

- Activate the environment:
```sh
conda activate TransGEM
```


# Download data

The data related to this study can be downloaded [here](https://ln5.sync.com/dl/31ed0ae00/9id3aj85-867mgggq-u3m3ksam-2enff2kh).

# Usage
### TransGEM training

- in subLINCS dataset


  File "/home/pcasarotto/miniconda3/envs/TransGEM/lib/python3.8/site-packages/torch_sparse/__init__.py", line 15, in <module>
    torch.ops.load_library(importlib.machinery.PathFinder().find_spec(
AttributeError: 'NoneType' object has no attribute 'origin'


```sh
python train.py --data_path ./data/  --dataset subLINCS --gene_encoder tenfold_binary --gpu cuda:0 --epochs 200

python train.py --data_path ./data/ --dataset subLINCS --gene_encoder tenfold_binary --gpu cuda:0 --epochs 200 | tee output_train_biomal.txt
python train.py --data_path ./data/ --dataset subLINCS --gene_encoder binary --gpu cuda:0 --epochs 200 | tee output_train_biomal_binary.txt
python train.py --data_path ./data/ --dataset subLINCS --gene_encoder one_hot --gpu cuda:0 --epochs 200 | tee output_train_biomal_one_hot.txt
python train.py --data_path ./data/ --dataset subLINCS --gene_encoder value --gpu cuda:0 --epochs 200 | tee output_train_biomal_value.txt

```

``` sh
python train.py --data_path ./data/ --dataset FAME_dataset --gene_encoder value --gpu cuda:0 --decimal_points 6 --epochs 2 | tee FAME_dataset_train.txt

python test.py --data_path ./data/  --dataset FAME_dataset --gene_encoder value --gpu cuda:0 --decimal_points 6

```

- in HCC515 dataset

```sh
python train.py --data_path ./data/  --dataset HCC515 --gene_encoder tenfold_binary --gpu cuda:0 --epochs 200
```

### TransGEM fine-tuning

```sh
python ft_train.py --data_path ./data/  --dataset HCC515 --gene_encoder tenfold_binary --gpu cuda:0
```

### Trained TransGEM testing

```sh
python test.py --data_path ./data/  --dataset subLINCS --gene_encoder tenfold_binary --gpu cuda:0 | tee test_results_biomal_tenfold_binary.txt

python test.py --data_path ./data/  --dataset subLINCS --gene_encoder binary --gpu cuda:0 | tee test_results_biomal_binary.txt

python test.py --data_path ./data/  --dataset subLINCS --gene_encoder one_hot --gpu cuda:0 | tee test_results_biomal_one_hot.txt

python test.py --data_path ./data/  --dataset subLINCS --gene_encoder value --gpu cuda:0 | tee test_results_biomal_value.txt
```

### Fine-tuned TransGEM testing

```sh
python ft_test.py --data_path ./data/  --dataset HCC515 --gene_encoder tenfold_binary --gpu cuda:0
```

### TransGEM application

- for Prostate cancer

```sh
python app.py --data_path ./data/  --dataset PC --cell_line PC3 --gene_encoder tenfold_binary --gpu cuda:0 --seq_num 1000
```

- for Non-small cell lung cancer

```sh
python app.py --data_path ./data/  --dataset nsclc --cell_line A549 --gene_encoder tenfold_binary --gpu cuda:0 --seq_num 1000
```

### Get attention scores between 978 genes and generated molecules

```sh
python get_attention.py --data_path ./data/  --gene_encoder tenfold_binary --gpu cuda:0
```

## Model options

- usage: 

```sh
python train.py --help
```

- optional arguments:

```console
-h, --help            show this help message and exit
--data_path           the directory where the data was inputted
--out_path            the directory where the trianing results was output
--dataset             the dataset used by the model (subLINCS/HCC515/PC/nsclc)
--gene_encoder        encoding form of gene expression (value/one_hot/binary/tenfold_binary)
--gpu                 CUDA device ids
--hidden_dim          hidden size of transformer decoder
--ff_dim              dimension number of the feed-forward layer
--PE_dropout          dropout of position coding
--TF_dropout          dropout of transformer layer
--TF_N                number of transformer decoder layer
--TF_H                number of transformer decoder head
--TF_act              activation function of transformer layer
--batch_size          number of batch_size
--epochs              number of epochs
--lr                  learning rate of adam
--cell_line           cell line names of disease
--pad_idx             id of pad symbol
--start_idx           id of start symbol
--end_idx             id of end symbol
--max_len             maximum length of generated molecule
--vocab_size          vocab size
--k                   number of molecules generated in a single beam search
--alpha               the weight of the length and score of molecules generated by bundle search
--seq_num             number of molecules ultimately retained
```

- Model parameters of 4 encoding forms

| Encoding form |  hidden_dim   |     ff_dim    |  TF_N         |  TF_H         |
| :------------:| :------------:| :------------:| :------------:| :------------:|
| value         | 64            |2048           |6              |8              |
| one_hot       | 64            |512            |6              |8              |
| binary        | 64            |512            |6              |8              |
| tenfold_binary| 64            |512            |6              |8              |
